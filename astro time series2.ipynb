{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# JWST Time-Series Analysis Lab v2  \n",
        "## Gaussian-Process systematics (celerite2 / george) + Transit Spectroscopy (batman) + Imaging TSO photometry (rateints / calints)\n",
        "\n",
        "This notebook extends the previous MAST+astroquery workflow with three \u201cspace-special\u201d upgrades:\n",
        "\n",
        "1. **Replace simple detrending with Gaussian Processes** for instrument systematics (prefer **celerite2**, optional **george** fallback).  \n",
        "2. **Fit a transit model (batman) jointly across wavelength bins** to produce a **transmission spectrum** (Rp/Rs vs wavelength).  \n",
        "3. For **imaging TSOs** (`rateints` / `calints`), perform **aperture photometry per integration** (similar to differential photometry workflows).\n",
        "\n",
        "Notes:\n",
        "- JWST time stamps often live in the `INT_TIMES` table; a common mid-time column is `int_mid_MJD_UTC`.  \n",
        "- Product availability varies by program/mode; this notebook tries **WHTLT \u2192 X1DINTS \u2192 RATEINTS/CALINTS** in that order.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# If you're missing dependencies, uncomment and run:\n",
        "# !pip -q install astroquery astropy numpy matplotlib scipy pandas\n",
        "# !pip -q install celerite2 george\n",
        "# !pip -q install batman-package\n",
        "# !pip -q install photutils\n",
        "\n",
        "print(\"Ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Imports\n",
        "# -----------------------------\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import astropy.units as u\n",
        "from astropy.io import fits\n",
        "from astropy.table import Table\n",
        "from astropy.stats import sigma_clip\n",
        "from astropy.timeseries import LombScargle\n",
        "\n",
        "from astroquery.mast import Observations\n",
        "\n",
        "# SciPy (optim + optional filters)\n",
        "try:\n",
        "    from scipy.optimize import minimize, least_squares\n",
        "    from scipy.signal import savgol_filter\n",
        "    HAS_SCIPY = True\n",
        "except Exception:\n",
        "    HAS_SCIPY = False\n",
        "\n",
        "# Photutils (recommended for imaging TSOs)\n",
        "try:\n",
        "    from photutils.aperture import CircularAperture, CircularAnnulus, aperture_photometry\n",
        "    HAS_PHOTUTILS = True\n",
        "except Exception:\n",
        "    HAS_PHOTUTILS = False\n",
        "\n",
        "# GP backends\n",
        "try:\n",
        "    import celerite2\n",
        "    from celerite2 import terms\n",
        "    from celerite2 import GaussianProcess\n",
        "    HAS_CELERITE2 = True\n",
        "except Exception:\n",
        "    HAS_CELERITE2 = False\n",
        "\n",
        "try:\n",
        "    import george\n",
        "    from george import kernels\n",
        "    HAS_GEORGE = True\n",
        "except Exception:\n",
        "    HAS_GEORGE = False\n",
        "\n",
        "# Transit model\n",
        "try:\n",
        "    import batman\n",
        "    HAS_BATMAN = True\n",
        "except Exception:\n",
        "    HAS_BATMAN = False\n",
        "\n",
        "# Optional transit search\n",
        "try:\n",
        "    from astropy.timeseries import BoxLeastSquares\n",
        "    HAS_BLS = True\n",
        "except Exception:\n",
        "    HAS_BLS = False\n",
        "\n",
        "print(\"HAS_SCIPY     :\", HAS_SCIPY)\n",
        "print(\"HAS_PHOTUTILS :\", HAS_PHOTUTILS)\n",
        "print(\"HAS_CELERITE2 :\", HAS_CELERITE2)\n",
        "print(\"HAS_GEORGE    :\", HAS_GEORGE)\n",
        "print(\"HAS_BATMAN    :\", HAS_BATMAN)\n",
        "print(\"HAS_BLS       :\", HAS_BLS)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Configuration\n",
        "# -----------------------------\n",
        "OUT_DIR = Path(\"jwst_timeseries_outputs_v2\")\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DOWNLOAD_DIR = OUT_DIR / \"mast_downloads\"\n",
        "DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Entry points: set either TARGET_NAME (+SEARCH_RADIUS) OR PROGRAM_ID\n",
        "TARGET_NAME = \"WASP-39\"\n",
        "SEARCH_RADIUS = 0.03 * u.deg\n",
        "\n",
        "PROGRAM_ID = None   # e.g. 1366\n",
        "\n",
        "# Choose observation row after query\n",
        "OBS_INDEX = 0\n",
        "\n",
        "# Product preference order (MAST productSubGroupDescription)\n",
        "PREFERRED_SUBGROUPS = [\"WHTLT\", \"X1DINTS\", \"RATEINTS\", \"CALINTS\"]\n",
        "\n",
        "# Spectroscopic binning (for X1DINTS)\n",
        "NBINS = 8\n",
        "WL_RANGE_MICRON = (None, None)  # e.g. (1.1, 1.7)\n",
        "\n",
        "# Imaging aperture photometry (pixels)\n",
        "AP_R    = 5.0\n",
        "ANN_RIN = 9.0\n",
        "ANN_ROUT= 14.0\n",
        "\n",
        "# Target centroid selection (imaging only)\n",
        "TARGET_MODE = \"auto\"  # \"auto\" or \"manual\"\n",
        "MANUAL_XY = (None, None)  # e.g. (x, y)\n",
        "\n",
        "# GP systematics settings\n",
        "GP_BACKEND = \"celerite2\"   # \"celerite2\" or \"george\" (auto-fallback if unavailable)\n",
        "GP_Q_FIXED = 1/np.sqrt(2)  # SHOTerm Q fixed (celerite2); reduces degeneracy\n",
        "GP_INIT_LOGS0 = np.log(1e-6)\n",
        "GP_INIT_LOGW0 = np.log(1.0)  # 1/days\n",
        "GP_INIT_LOGJITTER = np.log(1e-4)\n",
        "\n",
        "# Transit model settings (batman)\n",
        "LD_LAW = \"quadratic\"\n",
        "LD_U = [0.1, 0.3]   # limb-darkening coefficients (placeholder; tune for instrument+star)\n",
        "SUPERSAMPLE = 1     # increase if cadence is coarse\n",
        "EXP_TIME_DAYS = 0.0 # set if you want batman exposure time integration (days), else 0\n",
        "\n",
        "# Joint fit: which shared params to fit (keep simple by default)\n",
        "FIT_SHARED = dict(t0=True, per=False, a=False, inc=False)\n",
        "# Provide initial guesses (days, days, stellar radii units, degrees)\n",
        "TRANSIT_INIT = dict(\n",
        "    t0=None,        # if None, inferred from median time\n",
        "    per=1.0,        # set your known period if available\n",
        "    rp=0.15,        # Rp/Rs (white-light initial)\n",
        "    a=10.0,         # a/Rs\n",
        "    inc=88.0,       # deg\n",
        "    ecc=0.0,\n",
        "    w=90.0\n",
        ")\n",
        "\n",
        "print(\"Output dir:\", OUT_DIR.resolve())\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Helpers: MAST query\n",
        "# -----------------------------\n",
        "def query_jwst():\n",
        "    if PROGRAM_ID is not None:\n",
        "        print(f\"Querying JWST by PROGRAM_ID={PROGRAM_ID} ...\")\n",
        "        return Observations.query_criteria(obs_collection=\"JWST\", proposal_id=str(PROGRAM_ID))\n",
        "\n",
        "    print(f\"Querying JWST near TARGET_NAME='{TARGET_NAME}' (radius={SEARCH_RADIUS}) ...\")\n",
        "    obs = Observations.query_object(TARGET_NAME, radius=SEARCH_RADIUS)\n",
        "    if \"obs_collection\" in obs.colnames:\n",
        "        obs = obs[obs[\"obs_collection\"] == \"JWST\"]\n",
        "    return obs\n",
        "\n",
        "obs = query_jwst()\n",
        "print(\"Total observations returned:\", len(obs))\n",
        "obs[:5]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Pick an observation\n",
        "# -----------------------------\n",
        "if len(obs) == 0:\n",
        "    raise RuntimeError(\"No observations found. Try a different TARGET_NAME / SEARCH_RADIUS or set PROGRAM_ID.\")\n",
        "\n",
        "cols = [c for c in [\"obs_id\",\"target_name\",\"instrument_name\",\"filters\",\"dataproduct_type\",\"t_min\",\"t_max\",\"proposal_id\"] if c in obs.colnames]\n",
        "if \"t_min\" in obs.colnames:\n",
        "    obs = obs[np.argsort(obs[\"t_min\"])]\n",
        "\n",
        "print(\"Preview columns:\", cols)\n",
        "(obs[cols][:20] if cols else obs[:20])\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Products + download\n",
        "# -----------------------------\n",
        "obs_row = obs[[OBS_INDEX]]\n",
        "print(\"Selected obs_id:\", obs_row[\"obs_id\"][0] if \"obs_id\" in obs_row.colnames else \"(unknown)\")\n",
        "\n",
        "products = Observations.get_product_list(obs_row)\n",
        "print(\"Products:\", len(products))\n",
        "\n",
        "if \"productSubGroupDescription\" in products.colnames:\n",
        "    products[\"productSubGroupDescription\"] = [str(x).upper() for x in products[\"productSubGroupDescription\"]]\n",
        "    uniq = sorted(set(products[\"productSubGroupDescription\"]))\n",
        "    print(\"Unique subgroups (first 60):\", uniq[:60])\n",
        "\n",
        "    wanted = Observations.filter_products(\n",
        "        products,\n",
        "        productSubGroupDescription=PREFERRED_SUBGROUPS,\n",
        "        mrp_only=False\n",
        "    )\n",
        "else:\n",
        "    wanted = products\n",
        "\n",
        "print(\"Filtered products:\", len(wanted))\n",
        "wanted[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Download\n",
        "if len(wanted) == 0:\n",
        "    raise RuntimeError(\"No products matched. Inspect 'products' and adjust PREFERRED_SUBGROUPS.\")\n",
        "\n",
        "manifest = Observations.download_products(\n",
        "    wanted,\n",
        "    download_dir=str(DOWNLOAD_DIR),\n",
        "    cache=True\n",
        ")\n",
        "print(\"Downloaded rows:\", len(manifest))\n",
        "manifest[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Helpers: locate downloaded files\n",
        "# -----------------------------\n",
        "def local_paths_from_manifest(manifest):\n",
        "    paths = []\n",
        "    for p in manifest[\"Local Path\"]:\n",
        "        if p and str(p).strip().lower() != \"none\":\n",
        "            paths.append(Path(p))\n",
        "    return [p for p in paths if p.exists()]\n",
        "\n",
        "local_paths = local_paths_from_manifest(manifest)\n",
        "print(\"Local files:\", len(local_paths))\n",
        "for p in local_paths[:15]:\n",
        "    print(\" -\", p.name)\n",
        "\n",
        "def pick_by_subgroup(paths, key):\n",
        "    key = key.lower()\n",
        "    return [p for p in paths if key in p.name.lower()]\n",
        "\n",
        "whtlt_files   = pick_by_subgroup(local_paths, \"whtlt\")\n",
        "x1dints_files = pick_by_subgroup(local_paths, \"x1dints\")\n",
        "rateints_files= pick_by_subgroup(local_paths, \"rateints\")\n",
        "calints_files = pick_by_subgroup(local_paths, \"calints\")\n",
        "\n",
        "print(\"Found:\", {\"whtlt\": len(whtlt_files), \"x1dints\": len(x1dints_files),\n",
        "                \"rateints\": len(rateints_files), \"calints\": len(calints_files)})\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Loaders: WHTLT / X1DINTS / Imaging cubes (RATEINTS/CALINTS)\n",
        "# -----------------------------\n",
        "def load_int_times(hdul):\n",
        "    # Try to load INT_TIMES and return mid-times if present\n",
        "    for hdu in hdul:\n",
        "        if isinstance(hdu, fits.BinTableHDU) and (\"INT_TIMES\" in hdu.name.upper()):\n",
        "            tab = Table(hdu.data)\n",
        "            # common JWST column name:\n",
        "            for cname in tab.colnames:\n",
        "                if cname.lower() == \"int_mid_mjd_utc\":\n",
        "                    return np.asarray(tab[cname], dtype=float), cname\n",
        "            # fallbacks\n",
        "            for cname in tab.colnames:\n",
        "                if (\"mid\" in cname.lower()) and (\"mjd\" in cname.lower()):\n",
        "                    return np.asarray(tab[cname], dtype=float), cname\n",
        "            for cname in tab.colnames:\n",
        "                if (\"mid\" in cname.lower()) and (\"bjd\" in cname.lower()):\n",
        "                    return np.asarray(tab[cname], dtype=float), cname\n",
        "            return None, None\n",
        "    return None, None\n",
        "\n",
        "def load_whtlt(path):\n",
        "    with fits.open(path, memmap=False) as hdul:\n",
        "        best = None\n",
        "        for hdu in hdul:\n",
        "            if not isinstance(hdu, fits.BinTableHDU):\n",
        "                continue\n",
        "            cols = [c.upper() for c in hdu.columns.names]\n",
        "            time_candidates = [c for c in cols if (\"TIME\" in c or \"MJD\" in c or \"BJD\" in c)]\n",
        "            flux_candidates = [c for c in cols if (\"FLUX\" in c and \"ERR\" not in c)]\n",
        "            if time_candidates and flux_candidates:\n",
        "                best = hdu\n",
        "                break\n",
        "        if best is None:\n",
        "            raise RuntimeError(\"Could not find a time/flux table in WHTLT file.\")\n",
        "\n",
        "        tab = Table(best.data)\n",
        "\n",
        "        tcol = next(c for c in tab.colnames if (\"TIME\" in c.upper() or \"MJD\" in c.upper() or \"BJD\" in c.upper()))\n",
        "        fcol = next(c for c in tab.colnames if (c.upper() == \"FLUX\" or ((\"FLUX\" in c.upper()) and (\"ERR\" not in c.upper()))))\n",
        "        ecol = None\n",
        "        for c in tab.colnames:\n",
        "            if \"ERR\" in c.upper() and \"FLUX\" in c.upper():\n",
        "                ecol = c\n",
        "                break\n",
        "\n",
        "        t = np.asarray(tab[tcol], dtype=float)\n",
        "        y = np.asarray(tab[fcol], dtype=float)\n",
        "        dy = np.asarray(tab[ecol], dtype=float) if ecol is not None else None\n",
        "        meta = {\"source\":\"WHTLT\",\"file\":str(path),\"tcol\":tcol,\"fcol\":fcol,\"ecol\":ecol}\n",
        "        return t, y, dy, meta, None\n",
        "\n",
        "def load_x1dints(path, wl_range_micron=(None,None)):\n",
        "    with fits.open(path, memmap=False) as hdul:\n",
        "        # times\n",
        "        t, tcol = load_int_times(hdul)\n",
        "\n",
        "        # extract1d table\n",
        "        h = None\n",
        "        for hdu in hdul:\n",
        "            if isinstance(hdu, fits.BinTableHDU) and (hdu.name.upper() == \"EXTRACT1D\"):\n",
        "                h = hdu\n",
        "                break\n",
        "        if h is None:\n",
        "            for hdu in hdul:\n",
        "                if not isinstance(hdu, fits.BinTableHDU):\n",
        "                    continue\n",
        "                cols = [c.upper() for c in hdu.columns.names]\n",
        "                if \"FLUX\" in cols and \"WAVELENGTH\" in cols:\n",
        "                    h = hdu\n",
        "                    break\n",
        "        if h is None:\n",
        "            raise RuntimeError(\"Could not find EXTRACT1D-like table with FLUX/WAVELENGTH in X1DINTS.\")\n",
        "\n",
        "        tab = Table(h.data)\n",
        "        wcol = next(c for c in tab.colnames if c.upper() == \"WAVELENGTH\")\n",
        "        fcol = next(c for c in tab.colnames if c.upper() == \"FLUX\")\n",
        "\n",
        "        wave = np.asarray(tab[wcol])\n",
        "        flux = np.asarray(tab[fcol])\n",
        "\n",
        "        # Normalize shapes: want flux2d = (nint, nwave), wave1d = (nwave,)\n",
        "        flux = np.asarray(flux)\n",
        "        wave = np.asarray(wave)\n",
        "\n",
        "        if flux.ndim == 1:\n",
        "            raise RuntimeError(\"FLUX is 1D; not integration-resolved. Try another file/obs.\")\n",
        "\n",
        "        if wave.ndim == 1:\n",
        "            if flux.shape[1] == wave.shape[0]:\n",
        "                flux2d = flux\n",
        "                wave1d = wave\n",
        "            elif flux.shape[0] == wave.shape[0]:\n",
        "                flux2d = flux.T\n",
        "                wave1d = wave\n",
        "            else:\n",
        "                flux2d = flux.reshape(flux.shape[0], -1)\n",
        "                wave1d = np.linspace(0, 1, flux2d.shape[1])\n",
        "        else:\n",
        "            wave1d = wave[0] if wave.ndim >= 2 else wave\n",
        "            flux2d = flux if flux.shape[1] == len(wave1d) else flux.T\n",
        "\n",
        "        nint, nw = flux2d.shape\n",
        "        if t is None:\n",
        "            t = np.arange(nint, dtype=float)\n",
        "            tcol = \"index\"\n",
        "\n",
        "        wmin, wmax = wl_range_micron\n",
        "        if wmin is None: wmin = float(np.nanmin(wave1d))\n",
        "        if wmax is None: wmax = float(np.nanmax(wave1d))\n",
        "        mask = (wave1d >= wmin) & (wave1d <= wmax)\n",
        "        if not np.any(mask):\n",
        "            raise RuntimeError(\"Wavelength mask empty; adjust WL_RANGE_MICRON.\")\n",
        "\n",
        "        # white-light curve\n",
        "        y = np.nansum(flux2d[:, mask], axis=1)\n",
        "        meta = {\"source\":\"X1DINTS\",\"file\":str(path),\"tcol\":tcol,\"wmin\":wmin,\"wmax\":wmax,\"nint\":nint,\"nwave\":nw}\n",
        "        return t, y, None, meta, {\"wave\": wave1d, \"flux2d\": flux2d}\n",
        "\n",
        "def _find_sci_hdu(hdul):\n",
        "    # Prefer named SCI extension, else first image extension\n",
        "    for hdu in hdul:\n",
        "        if isinstance(hdu, fits.ImageHDU) and hdu.name.upper() == \"SCI\":\n",
        "            return hdu\n",
        "    # primary can also be image\n",
        "    for hdu in hdul:\n",
        "        if isinstance(hdu, (fits.PrimaryHDU, fits.ImageHDU)) and (hdu.data is not None) and (np.asarray(hdu.data).ndim >= 2):\n",
        "            return hdu\n",
        "    return None\n",
        "\n",
        "def detect_integration_axis(arr):\n",
        "    arr = np.asarray(arr)\n",
        "    if arr.ndim < 3:\n",
        "        return None\n",
        "    # assume spatial axes are the two largest dimensions (common in imaging)\n",
        "    sizes = list(arr.shape)\n",
        "    spatial_axes = sorted(range(arr.ndim), key=lambda ax: sizes[ax], reverse=True)[:2]\n",
        "    non_spatial = [ax for ax in range(arr.ndim) if ax not in spatial_axes]\n",
        "    if not non_spatial:\n",
        "        return None\n",
        "    # integration axis often larger than other non-spatial (e.g., groups); choose largest among non-spatial\n",
        "    non_spatial.sort(key=lambda ax: sizes[ax], reverse=True)\n",
        "    return non_spatial[0], spatial_axes\n",
        "\n",
        "def iter_integration_frames(arr):\n",
        "    arr = np.asarray(arr)\n",
        "    iax, spatial_axes = detect_integration_axis(arr)\n",
        "    if iax is None:\n",
        "        raise RuntimeError(\"Could not detect integration axis in imaging cube.\")\n",
        "    # Move integration axis first\n",
        "    x = np.moveaxis(arr, iax, 0)\n",
        "    # Now need to reduce any remaining non-spatial axes (e.g., group axis) by selecting the last plane\n",
        "    while x.ndim > 3:\n",
        "        # take last index along axis=1 until shape is (nint, ny, nx) in some order\n",
        "        x = x[:, -1]\n",
        "    # Ensure last two axes are spatial: if not, permute by choosing two largest as spatial\n",
        "    if x.ndim != 3:\n",
        "        raise RuntimeError(\"Unexpected imaging cube shape after reduction.\")\n",
        "    # reorder to (nint, ny, nx)\n",
        "    sizes = list(x.shape)\n",
        "    spatial = sorted(range(1, 3), key=lambda ax: sizes[ax], reverse=True)  # among last two\n",
        "    # last two already spatial; return as is\n",
        "    return x\n",
        "\n",
        "def load_imaging_cube(path):\n",
        "    with fits.open(path, memmap=False) as hdul:\n",
        "        t, tcol = load_int_times(hdul)\n",
        "        sci = _find_sci_hdu(hdul)\n",
        "        if sci is None:\n",
        "            raise RuntimeError(\"Could not find SCI image data in file.\")\n",
        "        cube = np.asarray(sci.data)\n",
        "        cube3 = iter_integration_frames(cube)  # (nint, ny, nx)\n",
        "        nint = cube3.shape[0]\n",
        "        if t is None:\n",
        "            t = np.arange(nint, dtype=float)\n",
        "            tcol = \"index\"\n",
        "        meta = {\"source\":\"IMAGING\", \"file\":str(path), \"tcol\":tcol, \"shape\":cube3.shape}\n",
        "        return t, cube3, meta\n",
        "\n",
        "# Choose best available\n",
        "extra = None\n",
        "if len(whtlt_files) > 0:\n",
        "    t, y, dy, meta, extra = load_whtlt(whtlt_files[0])\n",
        "elif len(x1dints_files) > 0:\n",
        "    t, y, dy, meta, extra = load_x1dints(x1dints_files[0], WL_RANGE_MICRON)\n",
        "elif len(rateints_files) > 0:\n",
        "    t, cube3, meta = load_imaging_cube(rateints_files[0])\n",
        "    y = dy = None\n",
        "    extra = {\"cube3\": cube3}\n",
        "elif len(calints_files) > 0:\n",
        "    t, cube3, meta = load_imaging_cube(calints_files[0])\n",
        "    y = dy = None\n",
        "    extra = {\"cube3\": cube3}\n",
        "else:\n",
        "    raise RuntimeError(\"No WHTLT/X1DINTS/RATEINTS/CALINTS found for this observation.\")\n",
        "\n",
        "print(\"Loaded meta:\", meta)\n",
        "print(\"Time length:\", len(t), \"| min..max:\", float(np.nanmin(t)), float(np.nanmax(t)))\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# If imaging cube: aperture photometry per integration -> light curve\n",
        "# -----------------------------\n",
        "def sanitize_finite(img):\n",
        "    img = np.asarray(img, dtype=float)\n",
        "    finite = np.isfinite(img)\n",
        "    fill = np.nanmedian(img[finite]) if np.any(finite) else 0.0\n",
        "    return np.where(finite, img, fill)\n",
        "\n",
        "def auto_centroid_peak(img, box_halfsize=15):\n",
        "    img = sanitize_finite(img)\n",
        "    ny, nx = img.shape\n",
        "    cy, cx = ny//2, nx//2\n",
        "    y0, y1 = max(0, cy-box_halfsize), min(ny, cy+box_halfsize+1)\n",
        "    x0, x1 = max(0, cx-box_halfsize), min(nx, cx+box_halfsize+1)\n",
        "    sub = img[y0:y1, x0:x1]\n",
        "    iy, ix = np.unravel_index(np.argmax(sub), sub.shape)\n",
        "    return float(x0+ix), float(y0+iy)\n",
        "\n",
        "def aperture_photometry_manual(img, x0, y0, r, rin, rout):\n",
        "    img = sanitize_finite(img)\n",
        "    ny, nx = img.shape\n",
        "    yy, xx = np.indices((ny, nx))\n",
        "    rr = np.sqrt((xx-x0)**2 + (yy-y0)**2)\n",
        "    aper = rr <= r\n",
        "    ann = (rr >= rin) & (rr <= rout)\n",
        "    bkg = np.nanmedian(img[ann]) if np.any(ann) else 0.0\n",
        "    flux = np.nansum(img[aper] - bkg)\n",
        "    return flux, bkg\n",
        "\n",
        "def extract_lightcurve_from_cube(cube3, t, ap_r=5.0, ann_rin=9.0, ann_rout=14.0):\n",
        "    cube3 = np.asarray(cube3)\n",
        "    nint = cube3.shape[0]\n",
        "    frame0 = cube3[0]\n",
        "\n",
        "    if TARGET_MODE == \"manual\" and (MANUAL_XY[0] is not None) and (MANUAL_XY[1] is not None):\n",
        "        x0, y0 = float(MANUAL_XY[0]), float(MANUAL_XY[1])\n",
        "    else:\n",
        "        x0, y0 = auto_centroid_peak(frame0)\n",
        "\n",
        "    flux = np.zeros(nint, dtype=float)\n",
        "    bkg  = np.zeros(nint, dtype=float)\n",
        "\n",
        "    if HAS_PHOTUTILS:\n",
        "        positions = [(x0, y0)]\n",
        "        aper = CircularAperture(positions, r=ap_r)\n",
        "        ann  = CircularAnnulus(positions, r_in=ann_rin, r_out=ann_rout)\n",
        "        aper_area = aper.area\n",
        "        ann_area = ann.area\n",
        "\n",
        "        for i in range(nint):\n",
        "            img = sanitize_finite(cube3[i])\n",
        "            ap_tbl = aperture_photometry(img, aper)\n",
        "            an_tbl = aperture_photometry(img, ann)\n",
        "            ann_sum = float(an_tbl[\"aperture_sum\"][0])\n",
        "            ann_mean = ann_sum / ann_area\n",
        "            flux[i] = float(ap_tbl[\"aperture_sum\"][0]) - ann_mean * aper_area\n",
        "            bkg[i] = ann_mean\n",
        "    else:\n",
        "        for i in range(nint):\n",
        "            f, b = aperture_photometry_manual(cube3[i], x0, y0, ap_r, ann_rin, ann_rout)\n",
        "            flux[i] = f\n",
        "            bkg[i] = b\n",
        "\n",
        "    return np.asarray(t, dtype=float), flux, None, {\"x0\":x0,\"y0\":y0,\"ap_r\":ap_r,\"ann_rin\":ann_rin,\"ann_rout\":ann_rout}\n",
        "\n",
        "if meta.get(\"source\") == \"IMAGING\":\n",
        "    tt, yy, ddy, phot_meta = extract_lightcurve_from_cube(extra[\"cube3\"], t, AP_R, ANN_RIN, ANN_ROUT)\n",
        "    t, y, dy = tt, yy, ddy\n",
        "    meta.update({\"photometry\": phot_meta})\n",
        "    print(\"Aperture photometry centroid:\", phot_meta[\"x0\"], phot_meta[\"y0\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Plot raw light curve (any source)\n",
        "# -----------------------------\n",
        "t = np.asarray(t, dtype=float)\n",
        "y = np.asarray(y, dtype=float)\n",
        "dy = (np.asarray(dy, dtype=float) if dy is not None else None)\n",
        "\n",
        "y0 = np.nanmedian(y)\n",
        "yn = y / y0\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(t, yn, \".\", ms=3)\n",
        "plt.xlabel(f\"Time ({meta.get('tcol','time')})\")\n",
        "plt.ylabel(\"Normalized flux\")\n",
        "plt.title(f\"Raw light curve ({meta.get('source','?')})\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Clean (sigma-clip) and define a transit mask (optional)\n",
        "# -----------------------------\n",
        "# Outlier clipping\n",
        "cl = sigma_clip(yn, sigma=5.0, maxiters=5)\n",
        "mask_ok = ~cl.mask if hasattr(cl, \"mask\") else np.isfinite(cl)\n",
        "t_clean = t[mask_ok]\n",
        "y_clean = yn[mask_ok]\n",
        "dy_clean = (dy[mask_ok]/y0 if dy is not None else None)\n",
        "\n",
        "# Define a transit mask (exclude transit) if you know approximate ephemeris.\n",
        "# This is helpful for learning GP systematics using out-of-transit data.\n",
        "# If unknown, set TRANSIT_MASK = None and we fit everything.\n",
        "if TRANSIT_INIT[\"t0\"] is None:\n",
        "    TRANSIT_INIT[\"t0\"] = float(np.nanmedian(t_clean))\n",
        "\n",
        "# Example: mask a window +/- 0.05 days around t0 (edit!)\n",
        "TRANSIT_HALF_WINDOW_DAYS = 0.05\n",
        "TRANSIT_MASK = np.abs(t_clean - TRANSIT_INIT[\"t0\"]) > TRANSIT_HALF_WINDOW_DAYS  # True = keep\n",
        "\n",
        "print(\"Clean points:\", len(t_clean))\n",
        "print(\"Transit mask keeps:\", int(np.sum(TRANSIT_MASK)), \"points\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# GP systematics model (celerite2 preferred; george fallback)\n",
        "# We fit: y(t) = mean_model(t) + GP(t) + noise\n",
        "# Here mean_model defaults to a constant 1.0 (normalized flux baseline)\n",
        "# -----------------------------\n",
        "def gp_systematics_fit(t, y, yerr=None, mean_model=None, mask=None):\n",
        "    t = np.asarray(t, dtype=float)\n",
        "    y = np.asarray(y, dtype=float)\n",
        "    if yerr is None:\n",
        "        yerr = 1e-3 * np.ones_like(y)\n",
        "    else:\n",
        "        yerr = np.asarray(yerr, dtype=float)\n",
        "\n",
        "    if mean_model is None:\n",
        "        mean_model = lambda tt: np.ones_like(tt)\n",
        "\n",
        "    if mask is None:\n",
        "        mask = np.isfinite(t) & np.isfinite(y) & np.isfinite(yerr)\n",
        "    else:\n",
        "        mask = np.asarray(mask, dtype=bool) & np.isfinite(t) & np.isfinite(y) & np.isfinite(yerr)\n",
        "\n",
        "    tt = t[mask]\n",
        "    yy = y[mask]\n",
        "    ee = yerr[mask]\n",
        "    # stabilize large MJDs\n",
        "    t0 = np.nanmin(tt)\n",
        "    x = tt - t0\n",
        "\n",
        "    backend = GP_BACKEND\n",
        "    if backend == \"celerite2\" and not HAS_CELERITE2 and HAS_GEORGE:\n",
        "        backend = \"george\"\n",
        "    if backend == \"george\" and not HAS_GEORGE and HAS_CELERITE2:\n",
        "        backend = \"celerite2\"\n",
        "\n",
        "    if backend == \"celerite2\" and HAS_CELERITE2:\n",
        "        def nll(p):\n",
        "            logS0, logw0, logjit = p\n",
        "            jitter = np.exp(logjit)\n",
        "            kernel = terms.SHOTerm(S0=np.exp(logS0), w0=np.exp(logw0), Q=GP_Q_FIXED)\n",
        "            gp = GaussianProcess(kernel, mean=0.0)\n",
        "            gp.compute(x, yerr=np.sqrt(ee**2 + jitter**2))\n",
        "            resid = yy - mean_model(tt)\n",
        "            return -gp.log_likelihood(resid)\n",
        "\n",
        "        p0 = np.array([GP_INIT_LOGS0, GP_INIT_LOGW0, GP_INIT_LOGJITTER], dtype=float)\n",
        "        sol = minimize(nll, p0, method=\"L-BFGS-B\")\n",
        "        logS0, logw0, logjit = sol.x\n",
        "        jitter = np.exp(logjit)\n",
        "        kernel = terms.SHOTerm(S0=np.exp(logS0), w0=np.exp(logw0), Q=GP_Q_FIXED)\n",
        "        gp = GaussianProcess(kernel, mean=0.0)\n",
        "        gp.compute(x, yerr=np.sqrt(ee**2 + jitter**2))\n",
        "        resid = yy - mean_model(tt)\n",
        "        mu = gp.predict(resid, x, return_cov=False)\n",
        "        # Build full-length prediction (NaN elsewhere)\n",
        "        mu_full = np.full_like(t, np.nan, dtype=float)\n",
        "        mu_full[mask] = mu\n",
        "        meta = {\"backend\":\"celerite2\",\"logS0\":float(logS0),\"logw0\":float(logw0),\"logjitter\":float(logjit),\"success\":bool(sol.success)}\n",
        "        return mu_full, meta\n",
        "\n",
        "    if backend == \"george\" and HAS_GEORGE:\n",
        "        # Simple ExpSquared kernel as a generic systematics model\n",
        "        def nll(p):\n",
        "            log_amp, log_tau, logjit = p\n",
        "            amp = np.exp(log_amp)\n",
        "            tau = np.exp(log_tau)\n",
        "            jitter = np.exp(logjit)\n",
        "            k = amp * kernels.ExpSquaredKernel(tau**2)\n",
        "            gp = george.GP(k)\n",
        "            gp.compute(x, np.sqrt(ee**2 + jitter**2))\n",
        "            resid = yy - mean_model(tt)\n",
        "            return -gp.log_likelihood(resid)\n",
        "\n",
        "        p0 = np.array([np.log(1e-3), np.log(0.1), GP_INIT_LOGJITTER], dtype=float)\n",
        "        sol = minimize(nll, p0, method=\"L-BFGS-B\")\n",
        "        log_amp, log_tau, logjit = sol.x\n",
        "        amp, tau, jitter = np.exp(log_amp), np.exp(log_tau), np.exp(logjit)\n",
        "        k = amp * kernels.ExpSquaredKernel(tau**2)\n",
        "        gp = george.GP(k)\n",
        "        gp.compute(x, np.sqrt(ee**2 + jitter**2))\n",
        "        resid = yy - mean_model(tt)\n",
        "        mu, _ = gp.predict(resid, x, return_cov=True)\n",
        "        mu_full = np.full_like(t, np.nan, dtype=float)\n",
        "        mu_full[mask] = mu\n",
        "        meta = {\"backend\":\"george\",\"log_amp\":float(log_amp),\"log_tau\":float(log_tau),\"logjitter\":float(logjit),\"success\":bool(sol.success)}\n",
        "        return mu_full, meta\n",
        "\n",
        "    raise RuntimeError(\"No GP backend available. Install celerite2 or george.\")\n",
        "\n",
        "# Fit GP systematics using out-of-transit points (TRANSIT_MASK). \n",
        "# mean model is just 1.0 baseline here; we will fit transit separately later.\n",
        "try:\n",
        "    mu_sys, gp_meta = gp_systematics_fit(t_clean, y_clean, yerr=(dy_clean if dy_clean is not None else None),\n",
        "                                         mean_model=lambda tt: np.ones_like(tt),\n",
        "                                         mask=TRANSIT_MASK)\n",
        "    print(\"GP meta:\", gp_meta)\n",
        "except Exception as e:\n",
        "    mu_sys, gp_meta = None, None\n",
        "    print(\"GP fit skipped / failed:\", repr(e))\n",
        "\n",
        "if mu_sys is not None:\n",
        "    y_gp_corrected = y_clean - mu_sys + 1.0  # remove systematics, keep around baseline 1\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.plot(t_clean, y_clean, \".\", ms=3, label=\"clean\")\n",
        "    plt.plot(t_clean, mu_sys, \"-\", lw=2, label=\"GP systematics\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Flux (normalized)\")\n",
        "    plt.title(\"GP systematics fit (out-of-transit)\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.plot(t_clean, y_gp_corrected, \".\", ms=3)\n",
        "    plt.axhline(1.0, ls=\"--\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Flux (GP-corrected)\")\n",
        "    plt.title(\"Light curve after GP systematics correction\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Lomb\u2013Scargle on GP-corrected residuals (optional variability search)\n",
        "# -----------------------------\n",
        "yy = y_gp_corrected if (mu_sys is not None) else y_clean\n",
        "ok = np.isfinite(t_clean) & np.isfinite(yy)\n",
        "tt = t_clean[ok]\n",
        "ff = yy[ok] - np.nanmedian(yy[ok])\n",
        "\n",
        "# If MJD-like, subtract for stability\n",
        "tref = np.nanmin(tt)\n",
        "x = tt - tref\n",
        "\n",
        "min_period = 0.05\n",
        "max_period = 10.0\n",
        "freq = np.linspace(1/max_period, 1/min_period, 20000)\n",
        "\n",
        "ls = LombScargle(x, ff)\n",
        "power = ls.power(freq)\n",
        "best_f = freq[np.argmax(power)]\n",
        "best_period = 1/best_f\n",
        "\n",
        "print(\"Best LS period ~\", best_period, \"days\")\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(1/freq, power)\n",
        "plt.gca().invert_xaxis()\n",
        "plt.xlabel(\"Period (days)\")\n",
        "plt.ylabel(\"LS Power\")\n",
        "plt.title(\"Lomb\u2013Scargle Periodogram (GP-corrected residuals)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Transit model with batman (white-light fit)\n",
        "# We'll fit a simple transit to the GP-corrected light curve.\n",
        "# -----------------------------\n",
        "if not HAS_BATMAN or not HAS_SCIPY:\n",
        "    print(\"Need batman-package and scipy to fit transits. Skipping.\")\n",
        "else:\n",
        "    tfit = np.asarray(t_clean, dtype=float)\n",
        "    yfit = np.asarray(y_gp_corrected if mu_sys is not None else y_clean, dtype=float)\n",
        "    efit = (np.asarray(dy_clean, dtype=float) if dy_clean is not None else 1e-3*np.ones_like(yfit))\n",
        "\n",
        "    # Use days and stabilize time\n",
        "    t0ref = np.nanmin(tfit)\n",
        "    x = tfit - t0ref\n",
        "\n",
        "    def batman_model(x, p_shared, rp, u=LD_U):\n",
        "        params = batman.TransitParams()\n",
        "        params.t0 = float(p_shared[\"t0\"]) - t0ref\n",
        "        params.per = float(p_shared[\"per\"])\n",
        "        params.rp  = float(rp)\n",
        "        params.a   = float(p_shared[\"a\"])\n",
        "        params.inc = float(p_shared[\"inc\"])\n",
        "        params.ecc = float(p_shared.get(\"ecc\", 0.0))\n",
        "        params.w   = float(p_shared.get(\"w\", 90.0))\n",
        "        params.limb_dark = LD_LAW\n",
        "        params.u = list(u)\n",
        "\n",
        "        if EXP_TIME_DAYS and EXP_TIME_DAYS > 0 and SUPERSAMPLE > 1:\n",
        "            m = batman.TransitModel(params, x, supersample_factor=SUPERSAMPLE, exp_time=EXP_TIME_DAYS)\n",
        "        else:\n",
        "            m = batman.TransitModel(params, x)\n",
        "        return m.light_curve(params)\n",
        "\n",
        "    # Initial guess\n",
        "    p_shared = dict(TRANSIT_INIT)\n",
        "    if p_shared[\"t0\"] is None:\n",
        "        p_shared[\"t0\"] = float(np.nanmedian(tfit))\n",
        "    # per is often known; set TRANSIT_INIT['per'] accordingly for real data\n",
        "\n",
        "    def residuals_white(theta):\n",
        "        # theta = [t0, rp, a, inc, c0]\n",
        "        t0, rp, a, inc, c0 = theta\n",
        "        ps = dict(p_shared)\n",
        "        ps[\"t0\"], ps[\"a\"], ps[\"inc\"] = t0, a, inc\n",
        "        model = c0 * batman_model(x, ps, rp)\n",
        "        return (yfit - model) / efit\n",
        "\n",
        "    theta0 = np.array([p_shared[\"t0\"], p_shared[\"rp\"], p_shared[\"a\"], p_shared[\"inc\"], 1.0], dtype=float)\n",
        "    # mild bounds\n",
        "    lo = [theta0[0]-1.0, 0.001, 1.0, 60.0, 0.8]\n",
        "    hi = [theta0[0]+1.0, 0.5, 200.0, 90.0, 1.2]\n",
        "\n",
        "    sol = least_squares(residuals_white, theta0, bounds=(lo,hi))\n",
        "    t0_w, rp_w, a_w, inc_w, c0_w = sol.x\n",
        "\n",
        "    print(\"White-light fit:\")\n",
        "    print(\" t0 :\", t0_w)\n",
        "    print(\" rp :\", rp_w)\n",
        "    print(\" a  :\", a_w)\n",
        "    print(\" inc:\", inc_w)\n",
        "    print(\" c0 :\", c0_w)\n",
        "\n",
        "    p_shared_fit = dict(p_shared)\n",
        "    p_shared_fit.update({\"t0\": float(t0_w), \"a\": float(a_w), \"inc\": float(inc_w)})\n",
        "\n",
        "    m_white = c0_w * batman_model(x, p_shared_fit, rp_w)\n",
        "\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.plot(tfit, yfit, \".\", ms=3, label=\"data\")\n",
        "    plt.plot(tfit, m_white, \"-\", lw=2, label=\"batman fit\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Flux (GP-corrected, normalized)\")\n",
        "    plt.title(\"White-light transit fit\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Save white-light params\n",
        "    import pandas as pd\n",
        "    pd.DataFrame([{\n",
        "        \"t0\": t0_w, \"per\": p_shared_fit[\"per\"], \"rp\": rp_w, \"a\": a_w, \"inc\": inc_w,\n",
        "        \"c0\": c0_w, \"gp_backend\": (gp_meta.get(\"backend\") if gp_meta else None)\n",
        "    }]).to_csv(OUT_DIR/\"white_light_fit.csv\", index=False)\n",
        "    print(\"Saved:\", OUT_DIR/\"white_light_fit.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Build wavelength-binned light curves (X1DINTS only)\n",
        "# Then: common-mode correction using GP systematics from white-light,\n",
        "# and a JOINT transit fit across bins to estimate Rp/Rs(\u03bb) (transmission spectrum).\n",
        "# -----------------------------\n",
        "if meta.get(\"source\") != \"X1DINTS\":\n",
        "    print(\"Not an X1DINTS dataset; skipping transmission spectrum steps.\")\n",
        "elif not (HAS_BATMAN and HAS_SCIPY):\n",
        "    print(\"Need batman + scipy. Skipping.\")\n",
        "else:\n",
        "    wave = np.asarray(extra[\"wave\"], dtype=float)\n",
        "    flux2d = np.asarray(extra[\"flux2d\"], dtype=float)  # (nint, nwave)\n",
        "    # Restrict to clean indices (mask_ok computed earlier on white-light)\n",
        "    idx_all = np.arange(len(t))\n",
        "    idx_clean = idx_all[mask_ok]\n",
        "    t_bin = t_clean\n",
        "    x = t_bin - np.nanmin(t_bin)\n",
        "\n",
        "    flux2d_c = flux2d[idx_clean, :]\n",
        "\n",
        "    # Define wavelength bins\n",
        "    wmin, wmax = WL_RANGE_MICRON\n",
        "    if wmin is None: wmin = float(np.nanmin(wave))\n",
        "    if wmax is None: wmax = float(np.nanmax(wave))\n",
        "    wmask = (wave >= wmin) & (wave <= wmax)\n",
        "    w_use = wave[wmask]\n",
        "\n",
        "    edges = np.linspace(np.nanmin(w_use), np.nanmax(w_use), NBINS+1)\n",
        "    centers = 0.5*(edges[:-1] + edges[1:])\n",
        "\n",
        "    # White-light GP systematics vector on clean points (mu_sys)\n",
        "    # If no GP, use zeros (no correction)\n",
        "    sys = mu_sys if mu_sys is not None else np.zeros_like(t_clean)\n",
        "\n",
        "    # Construct binned light curves\n",
        "    Y = []\n",
        "    for i in range(NBINS):\n",
        "        m = (wave >= edges[i]) & (wave < edges[i+1])\n",
        "        m = m & wmask\n",
        "        if not np.any(m):\n",
        "            Y.append(np.full_like(t_bin, np.nan))\n",
        "            continue\n",
        "        lc = np.nansum(flux2d_c[:, m], axis=1)\n",
        "        lc /= np.nanmedian(lc)\n",
        "        # common-mode systematics correction (subtract systematics and re-add baseline)\n",
        "        lc_corr = lc - sys + 1.0\n",
        "        Y.append(lc_corr)\n",
        "\n",
        "    Y = np.array(Y)  # (nbin, ntime)\n",
        "\n",
        "    # Simple per-point error proxy\n",
        "    E = 1e-3*np.ones_like(Y)\n",
        "\n",
        "    # Joint fit parameters:\n",
        "    # shared: (t0, [optional per,a,inc]) and per-bin rp and per-bin baseline c0\n",
        "    # We'll use the white-light fitted shared parameters if available.\n",
        "    # If previous cell wasn't run, fall back to TRANSIT_INIT.\n",
        "    try:\n",
        "        t0_shared = float(t0_w)\n",
        "        per_shared = float(p_shared_fit[\"per\"])\n",
        "        a_shared = float(a_w)\n",
        "        inc_shared = float(inc_w)\n",
        "    except Exception:\n",
        "        t0_shared = float(TRANSIT_INIT[\"t0\"])\n",
        "        per_shared = float(TRANSIT_INIT[\"per\"])\n",
        "        a_shared = float(TRANSIT_INIT[\"a\"])\n",
        "        inc_shared = float(TRANSIT_INIT[\"inc\"])\n",
        "\n",
        "    # Build batman model callable\n",
        "    def batman_curve(x, t0_abs, per, rp, a, inc, u=LD_U):\n",
        "        params = batman.TransitParams()\n",
        "        params.t0 = t0_abs - np.nanmin(t_bin)  # in the same shifted system as x\n",
        "        params.per = per\n",
        "        params.rp  = rp\n",
        "        params.a   = a\n",
        "        params.inc = inc\n",
        "        params.ecc = float(TRANSIT_INIT.get(\"ecc\", 0.0))\n",
        "        params.w   = float(TRANSIT_INIT.get(\"w\", 90.0))\n",
        "        params.limb_dark = LD_LAW\n",
        "        params.u = list(u)\n",
        "        if EXP_TIME_DAYS and EXP_TIME_DAYS > 0 and SUPERSAMPLE > 1:\n",
        "            m = batman.TransitModel(params, x, supersample_factor=SUPERSAMPLE, exp_time=EXP_TIME_DAYS)\n",
        "        else:\n",
        "            m = batman.TransitModel(params, x)\n",
        "        return m.light_curve(params)\n",
        "\n",
        "    # Parameter packing\n",
        "    # theta = [t0] + [rp_i]*NBINS + [c0_i]*NBINS\n",
        "    def pack(theta_t0, rps, c0s):\n",
        "        return np.concatenate([[theta_t0], np.asarray(rps), np.asarray(c0s)])\n",
        "\n",
        "    def unpack(theta):\n",
        "        theta = np.asarray(theta, dtype=float)\n",
        "        t0 = theta[0]\n",
        "        rps = theta[1:1+NBINS]\n",
        "        c0s = theta[1+NBINS:1+2*NBINS]\n",
        "        return t0, rps, c0s\n",
        "\n",
        "    # Initial guesses\n",
        "    rp0 = np.full(NBINS, float(rp_w) if \"rp_w\" in globals() else float(TRANSIT_INIT[\"rp\"]))\n",
        "    c00 = np.ones(NBINS, dtype=float)\n",
        "    theta0 = pack(t0_shared, rp0, c00)\n",
        "\n",
        "    # Bounds\n",
        "    lo = pack(t0_shared-0.2, np.full(NBINS, 0.001), np.full(NBINS, 0.8))\n",
        "    hi = pack(t0_shared+0.2, np.full(NBINS, 0.5),   np.full(NBINS, 1.2))\n",
        "\n",
        "    def resid_joint(theta):\n",
        "        t0, rps, c0s = unpack(theta)\n",
        "        res_all = []\n",
        "        for i in range(NBINS):\n",
        "            yi = Y[i]\n",
        "            ei = E[i]\n",
        "            ok = np.isfinite(yi) & np.isfinite(ei)\n",
        "            if not np.any(ok):\n",
        "                continue\n",
        "            model = c0s[i] * batman_curve(x[ok], t0, per_shared, rps[i], a_shared, inc_shared)\n",
        "            res_all.append((yi[ok] - model) / ei[ok])\n",
        "        if len(res_all) == 0:\n",
        "            return np.array([0.0])\n",
        "        return np.concatenate(res_all)\n",
        "\n",
        "    sol = least_squares(resid_joint, theta0, bounds=(lo, hi))\n",
        "    t0_j, rps_j, c0s_j = unpack(sol.x)\n",
        "\n",
        "    # Uncertainty estimate (rough): diagonal from (J^T J)^-1 scaled by residual variance\n",
        "    rp_err = np.full(NBINS, np.nan)\n",
        "    try:\n",
        "        J = sol.jac\n",
        "        # covariance ~ s^2 * (J^T J)^-1\n",
        "        _, svals, VT = np.linalg.svd(J, full_matrices=False)\n",
        "        tol = np.finfo(float).eps * max(J.shape) * svals[0]\n",
        "        svals = svals[svals > tol]\n",
        "        VT = VT[:len(svals)]\n",
        "        cov = (VT.T / (svals**2)) @ VT\n",
        "        dof = max(1, len(sol.fun) - len(sol.x))\n",
        "        s2 = 2*np.sum(sol.fun**2) / dof\n",
        "        cov *= s2\n",
        "        rp_err = np.sqrt(np.diag(cov)[1:1+NBINS])\n",
        "    except Exception as e:\n",
        "        print(\"Could not estimate uncertainties:\", repr(e))\n",
        "\n",
        "    print(\"Joint fit done. t0 =\", t0_j, \"| per fixed =\", per_shared)\n",
        "\n",
        "    # Plot binned fits\n",
        "    plt.figure(figsize=(10,6))\n",
        "    offset = 0.0\n",
        "    for i in range(NBINS):\n",
        "        yi = Y[i]\n",
        "        ok = np.isfinite(yi)\n",
        "        if not np.any(ok):\n",
        "            continue\n",
        "        model = c0s_j[i] * batman_curve(x[ok], t0_j, per_shared, rps_j[i], a_shared, inc_shared)\n",
        "        plt.plot(t_bin[ok], yi[ok] + offset, \".\", ms=2)\n",
        "        plt.plot(t_bin[ok], model + offset, \"-\", lw=1.5)\n",
        "        plt.text(t_bin[ok][0], yi[ok][0] + offset, f\"{centers[i]:.2f} \u03bcm\", fontsize=8, va=\"bottom\")\n",
        "        offset += 0.03\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Flux (stacked)\")\n",
        "    plt.title(\"Spectroscopic bins: data + joint transit fits (after GP common-mode correction)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Transmission spectrum\n",
        "    depth = rps_j**2\n",
        "    depth_err = 2*rps_j*rp_err\n",
        "\n",
        "    import pandas as pd\n",
        "    spec = pd.DataFrame({\n",
        "        \"wl_center_micron\": centers,\n",
        "        \"rp_rs\": rps_j,\n",
        "        \"rp_rs_err\": rp_err,\n",
        "        \"depth\": depth,\n",
        "        \"depth_err\": depth_err\n",
        "    })\n",
        "    spec_path = OUT_DIR / \"transmission_spectrum.csv\"\n",
        "    spec.to_csv(spec_path, index=False)\n",
        "    print(\"Saved:\", spec_path)\n",
        "\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.errorbar(centers, rps_j, yerr=rp_err, fmt=\"o\")\n",
        "    plt.xlabel(\"Wavelength (micron)\")\n",
        "    plt.ylabel(\"Rp/Rs\")\n",
        "    plt.title(\"Transmission spectrum (Rp/Rs vs wavelength)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.errorbar(centers, depth, yerr=depth_err, fmt=\"o\")\n",
        "    plt.xlabel(\"Wavelength (micron)\")\n",
        "    plt.ylabel(\"Transit depth (Rp/Rs)^2\")\n",
        "    plt.title(\"Transmission spectrum (depth)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# -----------------------------\n",
        "# Optional: BLS transit search (useful if ephemeris unknown; best on white-light)\n",
        "# -----------------------------\n",
        "if not HAS_BLS:\n",
        "    print(\"BLS not available in this astropy; skipping.\")\n",
        "else:\n",
        "    # Use GP-corrected flux\n",
        "    y_use = y_gp_corrected if mu_sys is not None else y_clean\n",
        "    ok = np.isfinite(t_clean) & np.isfinite(y_use)\n",
        "    tt = t_clean[ok]\n",
        "    yy = y_use[ok]\n",
        "    ee = (dy_clean[ok] if dy_clean is not None else 1e-3*np.ones_like(yy))\n",
        "\n",
        "    tref = np.nanmin(tt)\n",
        "    x = tt - tref\n",
        "\n",
        "    periods = np.linspace(0.2, 10.0, 4000)\n",
        "    durations = np.linspace(0.005, 0.2, 25)  # days\n",
        "\n",
        "    bls = BoxLeastSquares(x, yy, dy=ee)\n",
        "    res = bls.power(periods, durations)\n",
        "    i_best = np.argmax(res.power)\n",
        "    p_best = res.period[i_best]\n",
        "    d_best = res.duration[i_best]\n",
        "    t0_best = res.transit_time[i_best]\n",
        "\n",
        "    print(\"Best BLS period:\", float(p_best), \"days | duration:\", float(d_best), \"days\")\n",
        "\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.plot(res.period, res.power)\n",
        "    plt.xlabel(\"Period (days)\")\n",
        "    plt.ylabel(\"BLS Power\")\n",
        "    plt.title(\"BLS Periodogram\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    phase = ((x - t0_best + 0.5*p_best) % p_best) / p_best - 0.5\n",
        "    order = np.argsort(phase)\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.plot(phase[order], yy[order], \".\", ms=2)\n",
        "    plt.axvspan(-0.5*float(d_best/p_best), 0.5*float(d_best/p_best), alpha=0.2)\n",
        "    plt.xlabel(\"Phase (centered)\")\n",
        "    plt.ylabel(\"Flux\")\n",
        "    plt.title(\"Folded at best BLS period\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical upgrades you can add next\n",
        "\n",
        "- Replace the simple uncertainty model with JWST-provided variance arrays (`VAR_POISSON`, `VAR_RNOISE`) where available in `rateints`.  \n",
        "- Fit **GP + transit simultaneously** (instead of common-mode GP then transit), using MCMC (e.g., `emcee`) if you want robust uncertainties.  \n",
        "- Use mode-specific limb darkening (e.g., `ExoTiC-LD`, `ldtk`, or tables) for more realistic transmission spectra.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}